# ğŸ“ Post-Mortem: Incidente de LatÃªncia na payment-api

---

## ğŸ“… Contexto

Em **13/07/2025 Ã s 08:03**, um alerta foi disparado no **Datadog** indicando que a **latÃªncia mÃ©dia da `payment-api`** estava acima de 2 segundos por mais de 10 minutos.

Esse serviÃ§o Ã© crÃ­tico para o processamento de pagamentos no e-commerce da empresa.

---

## ğŸ•’ Linha do Tempo

| HorÃ¡rio | Evento |
|--------|--------|
| 08:03  | Alerta crÃ­tico emitido no Datadog |
| 08:05  | InÃ­cio da investigaÃ§Ã£o pelo time de SRE |
| 08:10  | Identificados timeouts em conexÃµes com o banco RDS |
| 08:15  | Escalonamento das rÃ©plicas de 6 para 10 |
| 08:20  | DBA confirma esgotamento de conexÃµes simultÃ¢neas no RDS |
| 08:30  | Realizado rollout restart do deployment `payment-api` |
| 08:40  | LatÃªncia normalizada |
| 08:50  | ComunicaÃ§Ã£o de resoluÃ§Ã£o enviada no canal de incidentes |

---

## ğŸ§¨ Causa Raiz

- O **pool de conexÃµes com o RDS foi esgotado**, gerando timeouts na aplicaÃ§Ã£o.
- O aumento inesperado de requisiÃ§Ãµes simultÃ¢neas nÃ£o foi absorvido pelo nÃºmero atual de rÃ©plicas.

---

## ğŸ“‰ Impacto

- ServiÃ§o de pagamentos com falhas e lentidÃ£o durante **~30 minutos**.
- Parte das transaÃ§Ãµes foi **cancelada automaticamente por timeout**.
- Perda de receita estimada nÃ£o foi mensurada no momento do post-mortem.

---

## ğŸ“š LiÃ§Ãµes Aprendidas

- ConexÃµes com banco de dados sÃ£o um **gargalo crÃ­tico** e devem ser monitoradas com mais granularidade.
- Alertas proativos para **uso de conexÃµes e saturaÃ§Ã£o de pool** estavam ausentes.
- SREs responderam rapidamente com boas prÃ¡ticas de mitigaÃ§Ã£o.

---

## ğŸ› ï¸ AÃ§Ãµes Preventivas

- [x] Criar alertas especÃ­ficos para **erros de conexÃ£o** com o RDS.
- [x] Monitorar `DatabaseConnections` e `DatabaseConnectionsLimit` via CloudWatch e Datadog.
- [x] Definir **SLOs para latÃªncia e disponibilidade da `payment-api`**.
- [x] Adicionar mÃ©tricas customizadas na aplicaÃ§Ã£o via OpenTelemetry.
- [ ] Revisar capacidade do RDS e escalabilidade horizontal.
- [ ] Atualizar documentaÃ§Ã£o com **playbooks de resposta a incidentes**.

---

## âœ… ConclusÃ£o

O incidente foi **rapidamente mitigado** com o aumento das rÃ©plicas e reinÃ­cio do serviÃ§o.  
Foram identificados **pontos cegos na observabilidade** e no controle de conexÃµes com o banco, que jÃ¡ estÃ£o sendo tratados com aÃ§Ãµes estruturais.

Esse aprendizado serÃ¡ compartilhado com os times de SRE e desenvolvimento para **evitar reincidÃªncia**.

---

*Post-mortem elaborado de forma blameless, com foco em melhoria contÃ­nua.*
